{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===== 1) Setup =====\n",
        "!pip -q install scikit-learn==1.7.2 pandas==2.2.2 numpy==1.26.4 joblib==1.4.2 pyyaml==6.0.2\n",
        "\n",
        "import os, json, joblib, time, shutil, yaml, platform\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# ---- paths: upload your CSVs to /content or mount Drive and point here\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/train_test_data/genome_train_cleaned.csv\"   # <-- put your file here\n",
        "\n",
        "# ---- model version folder\n",
        "STAMP = time.strftime(\"v%Y-%m-%d_%H-%M\")\n",
        "TASK  = \"genetic_disorder\"\n",
        "EXPORT_DIR = f\"/content/{TASK}/{STAMP}\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "# ===== 2) Define EXACT 27 feature columns =====\n",
        "# These must match your frontend and backend schema EXACTLY (names and casing).\n",
        "FEATURE_COLS = [\n",
        "    \"Gender_ambiguous\", \"Gender_female\", \"Gender_male\",\n",
        "    \"Blood test result_abnormal\", \"Blood test result_inconclusive\",\n",
        "    \"Blood test result_normal\", \"Blood test result_slightly abnormal\",\n",
        "    \"Patient Age\", \"Blood cell count (mcL)\", \"Mother's age\", \"Father's age\",\n",
        "    \"No. of previous abortion\",\n",
        "    \"White Blood cell count (thousand per microliter)\",\n",
        "    \"Symptom 1\", \"Symptom 2\", \"Symptom 3\", \"Symptom 4\", \"Symptom 5\",\n",
        "    \"Parental Age Diff\", \"Symptom Score\",\n",
        "    \"Genes in mother's side\", \"Inherited from father\",\n",
        "    \"Maternal gene\", \"Paternal gene\",\n",
        "    \"Status\", \"Respiratory Rate (breaths/min)\",\n",
        "    \"Heart Rate (rates/min)\"  # <-- ensure the closing parenthesis is present\n",
        "]\n",
        "\n",
        "TARGET_PARENT = \"Genetic Disorder\"\n",
        "TARGET_CHILD  = \"Disorder Subclass\"\n",
        "\n",
        "# ===== 3) Load & clean =====\n",
        "df = pd.read_csv(TRAIN_CSV)\n",
        "\n",
        "# Keep only the needed columns (drop anything else)\n",
        "needed = FEATURE_COLS + [TARGET_PARENT, TARGET_CHILD]\n",
        "missing = [c for c in needed if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Columns missing from CSV: {missing}\")\n",
        "\n",
        "df = df[needed].copy()\n",
        "\n",
        "# Enforce numeric for feature cols\n",
        "for c in FEATURE_COLS:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Drop rows with any NaN in features or targets (prevents 'nan' becoming a class)\n",
        "df = df.dropna(subset=FEATURE_COLS + [TARGET_PARENT, TARGET_CHILD]).reset_index(drop=True)\n",
        "\n",
        "# (Optional) constrain targets to expected sets (if you want to enforce clean classes)\n",
        "PARENT_ALLOWED = [\n",
        "    \"Mitochondrial genetic inheritance disorders\",\n",
        "    \"Multifactorial genetic inheritance disorders\",\n",
        "    \"Single-gene inheritance diseases\"\n",
        "]\n",
        "CHILD_ALLOWED = [\n",
        "    \"Cancer\", \"Cystic fibrosis\", \"Diabetes\", \"Down syndrome\",\n",
        "    \"Huntington's disease\", \"Klinefelter syndrome\",\n",
        "    \"Leber's hereditary optic neuropathy\", \"Leigh syndrome\", \"Turner syndrome\"\n",
        "]\n",
        "df = df[df[TARGET_PARENT].isin(PARENT_ALLOWED) & df[TARGET_CHILD].isin(CHILD_ALLOWED)].reset_index(drop=True)\n",
        "\n",
        "X = df[FEATURE_COLS].astype(float)\n",
        "y_parent_str = df[TARGET_PARENT].astype(str)\n",
        "y_child_str  = df[TARGET_CHILD].astype(str)\n",
        "\n",
        "# ===== 4) Label encoders (avoid np.nan as class) =====\n",
        "le_parent = LabelEncoder().fit(y_parent_str)\n",
        "le_child  = LabelEncoder().fit(y_child_str)\n",
        "\n",
        "y_parent = le_parent.transform(y_parent_str)\n",
        "y_child  = le_child.transform(y_child_str)\n",
        "\n",
        "# ===== 5) Train/val split =====\n",
        "X_train, X_val, yp_train, yp_val, yc_train, yc_val = train_test_split(\n",
        "    X, y_parent, y_child, test_size=0.2, random_state=42, stratify=y_parent\n",
        ")\n",
        "\n",
        "# ===== 6) Train models (fixed params, no CV) =====\n",
        "# Use robust, balanced-ish defaults; adjust if you like.\n",
        "parent_model = RandomForestClassifier(\n",
        "    n_estimators=400, max_depth=14, min_samples_split=2, min_samples_leaf=1,\n",
        "    n_jobs=-1, random_state=42, class_weight=None\n",
        ")\n",
        "child_model = RandomForestClassifier(\n",
        "    n_estimators=500, max_depth=18, min_samples_split=2, min_samples_leaf=1,\n",
        "    n_jobs=-1, random_state=42, class_weight=None\n",
        ")\n",
        "\n",
        "parent_model.fit(X_train, yp_train)\n",
        "child_model.fit(X_train, yc_train)\n",
        "\n",
        "# ===== 7) Evaluate (simple metrics) =====\n",
        "yp_pred = parent_model.predict(X_val)\n",
        "yc_pred = child_model.predict(X_val)\n",
        "\n",
        "metrics = {\n",
        "    \"parent\": {\n",
        "        \"accuracy\": float(accuracy_score(yp_val, yp_pred)),\n",
        "        \"f1_macro\": float(f1_score(yp_val, yp_pred, average=\"macro\")),\n",
        "        \"report\": classification_report(yp_val, yp_pred, output_dict=True, zero_division=0),\n",
        "    },\n",
        "    \"child\": {\n",
        "        \"accuracy\": float(accuracy_score(yc_val, yc_pred)),\n",
        "        \"f1_macro\": float(f1_score(yc_val, yc_pred, average=\"macro\")),\n",
        "        \"report\": classification_report(yc_val, yc_pred, output_dict=True, zero_division=0),\n",
        "    },\n",
        "}\n",
        "# ensure JSON-serializable keys (cast any np.int64 to str)\n",
        "def _to_jsonable(d):\n",
        "    if isinstance(d, dict):\n",
        "        return {str(k): _to_jsonable(v) for k, v in d.items()}\n",
        "    if isinstance(d, (np.floating, np.float32, np.float64)):\n",
        "        return float(d)\n",
        "    if isinstance(d, (np.integer, np.int32, np.int64)):\n",
        "        return int(d)\n",
        "    if isinstance(d, (list, tuple)):\n",
        "        return [_to_jsonable(x) for x in d]\n",
        "    return d\n",
        "metrics = _to_jsonable(metrics)\n",
        "\n",
        "# ===== 8) Minimal identity pipeline (so backend .transform() is safe) =====\n",
        "# We just save an identity object; the backend already aligns columns.\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class IdentityTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None): return self\n",
        "    def transform(self, X): return X\n",
        "\n",
        "pipeline = IdentityTransformer()\n",
        "\n",
        "# ===== 9) Export artifacts =====\n",
        "joblib.dump(parent_model, f\"{EXPORT_DIR}/parent_model.joblib\", compress=3)\n",
        "joblib.dump(child_model,  f\"{EXPORT_DIR}/child_model.joblib\",  compress=3)\n",
        "joblib.dump(le_parent,    f\"{EXPORT_DIR}/le_parent.joblib\",   compress=3)\n",
        "joblib.dump(le_child,     f\"{EXPORT_DIR}/le_child.joblib\",    compress=3)\n",
        "joblib.dump(pipeline,     f\"{EXPORT_DIR}/pipeline.joblib\",     compress=3)\n",
        "\n",
        "# schema.json (27 features)\n",
        "schema = {\n",
        "    \"features\": [{\"name\": c, \"dtype\": \"float\"} for c in FEATURE_COLS]\n",
        "}\n",
        "with open(f\"{EXPORT_DIR}/schema.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(schema, f, indent=2)\n",
        "\n",
        "# targets.json (string class names, no NaN)\n",
        "targets = {\n",
        "    \"targets\": [TARGET_PARENT, TARGET_CHILD],\n",
        "    \"classes\": {\n",
        "        TARGET_PARENT: [str(x) for x in le_parent.classes_.tolist()],\n",
        "        TARGET_CHILD:  [str(x) for x in le_child.classes_.tolist()],\n",
        "    }\n",
        "}\n",
        "with open(f\"{EXPORT_DIR}/targets.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(targets, f, indent=2)\n",
        "\n",
        "# metrics.json\n",
        "with open(f\"{EXPORT_DIR}/metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# model_info.yaml\n",
        "info = {\n",
        "    \"task\": TASK,\n",
        "    \"version\": STAMP,\n",
        "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"feature_count\": len(FEATURE_COLS),\n",
        "    \"sklearn_version\": \"1.7.2\",\n",
        "    \"python\": platform.python_version(),\n",
        "    \"params\": {\n",
        "        \"parent_model\": dict(n_estimators=400, max_depth=14, min_samples_split=2, min_samples_leaf=1, random_state=42),\n",
        "        \"child_model\":  dict(n_estimators=500, max_depth=18, min_samples_split=2, min_samples_leaf=1, random_state=42),\n",
        "    }\n",
        "}\n",
        "with open(f\"{EXPORT_DIR}/model_info.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.safe_dump(info, f, sort_keys=False)\n",
        "\n",
        "# bundle_meta.json (optional helper)\n",
        "bundle_meta = {\n",
        "    \"paths\": {\n",
        "        \"parent_model\": f\"{EXPORT_DIR}/parent_model.joblib\",\n",
        "        \"child_model\":  f\"{EXPORT_DIR}/child_model.joblib\",\n",
        "        \"le_parent\":    f\"{EXPORT_DIR}/le_parent.joblib\",\n",
        "        \"le_child\":     f\"{EXPORT_DIR}/le_child.joblib\",\n",
        "        \"pipeline\":     f\"{EXPORT_DIR}/pipeline.joblib\",\n",
        "        \"schema\":       f\"{EXPORT_DIR}/schema.json\",\n",
        "        \"targets\":      f\"{EXPORT_DIR}/targets.json\",\n",
        "        \"metrics\":      f\"{EXPORT_DIR}/metrics.json\",\n",
        "        \"model_info\":   f\"{EXPORT_DIR}/model_info.yaml\",\n",
        "    }\n",
        "}\n",
        "with open(f\"{EXPORT_DIR}/bundle_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(bundle_meta, f, indent=2)\n",
        "\n",
        "print(\"Exported to:\", EXPORT_DIR)\n",
        "\n",
        "# ===== 10) Make a zip for download =====\n",
        "ZIP_PATH = shutil.make_archive(f\"/content/{TASK}_{STAMP}\", \"zip\", root_dir=f\"/content/{TASK}\", base_dir=STAMP)\n",
        "ZIP_PATH"
      ],
      "metadata": {
        "id": "r1FDXQMd4f2g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "b217d583-63d7-4ed4-d3fc-e27820356cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1522942989.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "83c4fS5ZVtfe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}