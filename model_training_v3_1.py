# -*- coding: utf-8 -*-
"""Model_Training_v3.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12GtL-bXrArujPYwTHR9IPS-Xauewbsr1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold, cross_val_predict, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.multioutput import MultiOutputClassifier

# ---- GPU-capable LightGBM
from lightgbm import LGBMClassifier

train_path = "/content/drive/MyDrive/train_test_data/train_encoded.csv"
test_path  = "/content/drive/MyDrive/train_test_data/train_encoded.csv"


train_df = pd.read_csv(train_path)
test_df  = pd.read_csv(test_path)

TARGETS = ["Genetic Disorder", "Disorder Subclass"]

X_train = train_df.drop(columns=TARGETS)
Y_train = train_df[TARGETS].copy()

X_test  = test_df.copy()

numeric_cols = X_train.select_dtypes(include=["int64","float64"]).columns.tolist()

preprocessor = ColumnTransformer(
    transformers=[("num", StandardScaler(), numeric_cols)],
    remainder="passthrough"
)

def make_lgbm(gpu_first: bool = True) -> LGBMClassifier:
    """
    Try GPU (device_type='gpu'); if it fails at fit time,
    catch the exception and recreate a CPU model seamlessly.
    """
    common = dict(
        n_estimators=800,
        learning_rate=0.05,
        max_depth=-1,
        num_leaves=31,
        min_child_samples=20,
        subsample=0.9,
        colsample_bytree=0.9,
        reg_alpha=0.0,
        reg_lambda=0.0,
        class_weight="balanced",
        n_jobs=-1,
        random_state=42
    )
    if gpu_first:
        return LGBMClassifier(
            device_type="gpu",      # <<< use GPU
            max_bin=255,            # good default for GPU hist
            **common
        )
    else:
        return LGBMClassifier(device_type="cpu", **common)

base_lgb = make_lgbm(gpu_first=True)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

def evaluate_target(target_name: str):
    y = Y_train[target_name]

    # Try GPU; if it fails, switch to CPU and continue
    clf_try = base_lgb
    pipe = Pipeline([("prep", preprocessor), ("clf", clf_try)])
    try:
        y_pred_oof  = cross_val_predict(pipe, X_train, y, cv=cv, method="predict", n_jobs=-1)
        y_proba_oof = cross_val_predict(pipe, X_train, y, cv=cv, method="predict_proba", n_jobs=-1)
        using_gpu = getattr(pipe.named_steps["clf"], "get_params")().get("device_type", "cpu") == "gpu"
    except Exception as e:
        print(f"[LightGBM GPU unavailable -> falling back to CPU] Reason: {e}")
        pipe = Pipeline([("prep", preprocessor), ("clf", make_lgbm(gpu_first=False))])
        y_pred_oof  = cross_val_predict(pipe, X_train, y, cv=cv, method="predict", n_jobs=-1)
        y_proba_oof = cross_val_predict(pipe, X_train, y, cv=cv, method="predict_proba", n_jobs=-1)
        using_gpu = False

    dev_label = "GPU" if using_gpu else "CPU"
    print(f"\n=== {target_name} — OOF Classification Report (5-fold CV, LGBM {dev_label}, balanced) ===\n")
    print(classification_report(y, y_pred_oof, digits=4, zero_division=0))

    # --- Confusion Matrix
    classes = np.unique(np.concatenate([y.unique(), y_pred_oof]))
    cm = confusion_matrix(y, y_pred_oof, labels=classes)

    fig, ax = plt.subplots(figsize=(6, 5))
    im = ax.imshow(cm, interpolation="nearest", cmap=plt.cm.Blues)
    ax.figure.colorbar(im, ax=ax)
    ax.set(
        xticks=np.arange(len(classes)),
        yticks=np.arange(len(classes)),
        xticklabels=classes,
        yticklabels=classes,
        ylabel="True label",
        xlabel="Predicted label",
        title=f"Confusion Matrix — {target_name} (OOF, LGBM {dev_label})"
    )
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    thresh = cm.max() / 2.0
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], "d"),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.show()

    # --- ROC (OvR) from OOF probabilities
    if y_proba_oof is not None and y_proba_oof.ndim == 2 and y_proba_oof.shape[1] >= 2:
        # Fit once to get class order (legend only)
        pipe.fit(X_train, y)
        cls_order = pipe.named_steps["clf"].classes_

        fig = plt.figure(figsize=(7, 5))
        for k, cls in enumerate(cls_order):
            from sklearn.metrics import roc_curve, auc
            fpr, tpr, _ = roc_curve((y == cls).astype(int), y_proba_oof[:, k])
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, linewidth=2, label=f"{cls} (AUC={roc_auc:.2f})")
        plt.plot([0, 1], [0, 1], linestyle="--")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title(f"ROC Curves (OvR) — {target_name} (OOF, LGBM {dev_label})")
        plt.legend()
        plt.show()

for tgt in TARGETS:
    evaluate_target(tgt)

from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score

per_target_rows = []
for tgt in TARGETS:
    y = Y_train[tgt]
    pipe = Pipeline([("prep", preprocessor), ("clf", base_lgb)])
    try:
        y_pred_oof = cross_val_predict(pipe, X_train, y, cv=cv, method="predict", n_jobs=-1)
        using_gpu = pipe.named_steps["clf"].get_params().get("device_type", "cpu") == "gpu"
    except Exception:
        pipe = Pipeline([("prep", preprocessor), ("clf", make_lgbm(gpu_first=False))])
        y_pred_oof = cross_val_predict(pipe, X_train, y, cv=cv, method="predict", n_jobs=-1)
        using_gpu = False

    acc  = accuracy_score(y, y_pred_oof)
    bacc = balanced_accuracy_score(y, y_pred_oof)
    f1m  = f1_score(y, y_pred_oof, average="macro", zero_division=0)
    f1w  = f1_score(y, y_pred_oof, average="weighted", zero_division=0)
    per_target_rows.append({
        "target": tgt, "device": "GPU" if using_gpu else "CPU",
        "accuracy": acc, "balanced_accuracy": bacc, "f1_macro": f1m, "f1_weighted": f1w
    })

summary_df = pd.DataFrame(per_target_rows).round(4)
print("\nOOF summary per target (LGBM, GPU if available):")
print(summary_df.to_string(index=False))

mean_acc = float(np.mean(summary_df["accuracy"]))
print("\nOverall mean accuracy (OOF across targets): {:.4f}".format(mean_acc))

plt.figure(figsize=(8,5))
plt.bar(summary_df["target"], summary_df["accuracy"], edgecolor="black")
plt.axhline(mean_acc, linestyle="--", label=f"Mean = {mean_acc:.2f}")
plt.xticks(rotation=45, ha="right")
plt.ylabel("Accuracy (OOF)")
plt.title("Per-target OOF Accuracy and Overall Mean — LightGBM (GPU if available)")
plt.legend()
plt.tight_layout()
plt.show()

y_primary = Y_train["Genetic Disorder"]
pipe_gs = Pipeline([("prep", preprocessor), ("clf", make_lgbm(gpu_first=True))])

param_grid = {
    "clf__n_estimators": [600, 800, 1000, 1200],
    "clf__num_leaves": [31, 63, 127],
    "clf__max_depth": [-1, 12, 20, 30],
    "clf__min_child_samples": [10, 20, 40],
    "clf__subsample": [0.7, 0.9, 1.0],
    "clf__colsample_bytree": [0.7, 0.9, 1.0],
    "clf__reg_alpha": [0.0, 0.1, 0.5],
    "clf__reg_lambda": [0.0, 0.1, 0.5],
}

gs = GridSearchCV(
    estimator=pipe_gs,
    param_grid=param_grid,
    scoring="f1_macro",
    cv=cv,
    n_jobs=-1,
    verbose=0
)

try:
    gs.fit(X_train, y_primary)
    print("\nBest params (Genetic Disorder, macro-F1, LGBM GPU/CPU):", gs.best_params_)
    print("Best CV macro-F1:", round(gs.best_score_, 4))
    best_lgb = LGBMClassifier(
        device_type="gpu",  # try GPU for final model
        n_estimators=gs.best_params_["clf__n_estimators"],
        learning_rate=0.05,
        num_leaves=gs.best_params_["clf__num_leaves"],
        max_depth=gs.best_params_["clf__max_depth"],
        min_child_samples=gs.best_params_["clf__min_child_samples"],
        subsample=gs.best_params_["clf__subsample"],
        colsample_bytree=gs.best_params_["clf__colsample_bytree"],
        reg_alpha=gs.best_params_["clf__reg_alpha"],
        reg_lambda=gs.best_params_["clf__reg_lambda"],
        class_weight="balanced",
        n_jobs=-1,
        random_state=42
    )
except Exception as e:
    print(f"[LightGBM GPU unavailable in GridSearch -> using CPU] Reason: {e}")
    best_lgb = LGBMClassifier(
        device_type="cpu",
        n_estimators=800, learning_rate=0.05, num_leaves=31, max_depth=-1,
        min_child_samples=20, subsample=0.9, colsample_bytree=0.9,
        reg_alpha=0.0, reg_lambda=0.0, class_weight="balanced",
        n_jobs=-1, random_state=42
    )

